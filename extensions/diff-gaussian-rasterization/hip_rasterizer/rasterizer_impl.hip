// !!! This is a file automatically generated by hipify!!!
/*
 * Copyright (C) 2023, Inria
 * GRAPHDECO research group, https://team.inria.fr/graphdeco
 * All rights reserved.
 *
 * This software is free for non-commercial, research and evaluation use 
 * under the terms of the LICENSE.md file.
 *
 * For inquiries contact  george.drettakis@inria.fr
 */

#include "../hip_rasterizer/rasterizer_impl.h"
#include <iostream>
#include <fstream>
#include <algorithm>
#include <numeric>
#include <hip/hip_runtime.h>
#include "hip/hip_runtime.h"
#include <c10/hip/HIPStream.h>
#ifndef __HIP_PLATFORM_AMD__
#include "device_launch_parameters.h"
#endif
#include <hipcub/hipcub.hpp>
#include <hipcub/hipcub.hpp>
// Use thrust for prefix scan - more reliable on HIP than hipcub
#include <thrust/device_ptr.h>
#include <thrust/scan.h>
#define GLM_FORCE_CUDA
#include <glm/glm.hpp>

#ifdef __HIP_PLATFORM_AMD__
#include "../hip_rasterizer/hip_cooperative_groups.h"
#else
#include <hip/hip_cooperative_groups.h>
#endif
#ifndef __HIP_PLATFORM_AMD__
#include <cooperative_groups/reduce.h>
#endif
namespace cg = cooperative_groups;

#include "../hip_rasterizer/auxiliary.h"
#include "../hip_rasterizer/forward.h"
#include "../hip_rasterizer/backward.h"

// Helper function to find the next-highest bit of the MSB
// on the CPU.
uint32_t getHigherMsb(uint32_t n)
{
	uint32_t msb = sizeof(n) * 4;
	uint32_t step = msb;
	while (step > 1)
	{
		step /= 2;
		if (n >> msb)
			msb += step;
		else
			msb -= step;
	}
	if (n >> msb)
		msb++;
	return msb;
}

// Custom inclusive scan for HIP - hipcub has issues on AMD GPUs
// This is a simple block-wise scan followed by block sum propagation
#define SCAN_BLOCK_SIZE 256

__global__ void blockInclusiveScan(const uint32_t* input, uint32_t* output, uint32_t* blockSums, int n)
{
	__shared__ uint32_t shared[SCAN_BLOCK_SIZE];
	
	int tid = threadIdx.x;
	int gid = blockIdx.x * blockDim.x + threadIdx.x;
	
	// Load into shared memory
	shared[tid] = (gid < n) ? input[gid] : 0;
	__syncthreads();
	
	// Up-sweep (reduce) phase
	for (int offset = 1; offset < blockDim.x; offset *= 2) {
		int ai = tid;
		int bi = tid - offset;
		if (bi >= 0 && tid < blockDim.x) {
			shared[ai] += shared[bi];
		}
		__syncthreads();
	}
	
	// Write output
	if (gid < n) {
		output[gid] = shared[tid];
	}
	
	// Write block sum (last element of each block)
	if (tid == blockDim.x - 1 && blockSums != nullptr) {
		blockSums[blockIdx.x] = shared[tid];
	}
}

__global__ void addBlockSums(uint32_t* data, const uint32_t* blockSums, int n)
{
	int gid = blockIdx.x * blockDim.x + threadIdx.x;
	if (blockIdx.x > 0 && gid < n) {
		data[gid] += blockSums[blockIdx.x - 1];
	}
}

// Minimal test kernel to verify HIP kernel execution works
__global__ void testKernel(uint32_t* data, int n)
{
	// Unconditional write to first element using threadIdx.x == 0
	if (threadIdx.x == 0 && blockIdx.x == 0) {
		data[0] = 77;  // Magic value using simple indexed write
		data[1] = 77;
		data[2] = 77;
	}
}

// CPU-based inclusive scan - guaranteed to work on any GPU
// Slower but reliable since GPU scan libraries have issues on HIP
void customInclusiveScan(uint32_t* d_input, uint32_t* d_output, int n)
{
	if (n <= 0) return;
	
	// Allocate host memory
	uint32_t* h_data = new uint32_t[n];
	
	// Copy from device to host
	hipMemcpy(h_data, d_input, n * sizeof(uint32_t), hipMemcpyDeviceToHost);
	
	// DEBUG: Print first 10 input values
	std::cerr << "[HIP SCAN DEBUG] First 10 tiles_touched values (before scan): ";
	for (int i = 0; i < 10 && i < n; i++) {
		std::cerr << h_data[i] << " ";
	}
	std::cerr << std::endl;
	
	// Compute sum of first 1000 elements to check if data is reasonable
	uint32_t sum = 0;
	for (int i = 0; i < 1000 && i < n; i++) {
		sum += h_data[i];
	}
	std::cerr << "[HIP SCAN DEBUG] Sum of first 1000 tiles_touched: " << sum << std::endl;
	
	// Perform inclusive scan on CPU
	for (int i = 1; i < n; i++) {
		h_data[i] += h_data[i-1];
	}
	
	// Copy back to device
	hipMemcpy(d_output, h_data, n * sizeof(uint32_t), hipMemcpyHostToDevice);
	
	// Cleanup
	delete[] h_data;
}

// Wrapper method to call auxiliary coarse frustum containment test.
// Mark all Gaussians that pass it.
__global__ void checkFrustum(int P,
	const float* orig_points,
	const float* viewmatrix,
	const float* projmatrix,
	bool* present)
{
	int idx = blockIdx.x * blockDim.x + threadIdx.x;
	if (idx >= P)
		return;

	float3 p_view;
	present[idx] = in_frustum(idx, orig_points, viewmatrix, projmatrix, false, p_view);
}

// Generates one key/value pair for all Gaussian / tile overlaps. 
// Run once per Gaussian (1:N mapping).
__global__ void duplicateWithKeys(
	int P,
	const float2* points_xy,
	const float* depths,
	const uint32_t* offsets,
	uint64_t* gaussian_keys_unsorted,
	uint32_t* gaussian_values_unsorted,
	int* radii,
	dim3 grid)
{
	int idx = blockIdx.x * blockDim.x + threadIdx.x;
	if (idx >= P)
		return;

	// Generate no key/value pair for invisible Gaussians
	if (radii[idx] > 0)
	{
		// Find this Gaussian's offset in buffer for writing keys/values.
		uint32_t off = (idx == 0) ? 0 : offsets[idx - 1];
		uint2 rect_min, rect_max;

		getRect(points_xy[idx], radii[idx], rect_min, rect_max, grid);

		// For each tile that the bounding rect overlaps, emit a 
		// key/value pair. The key is |  tile ID  |      depth      |,
		// and the value is the ID of the Gaussian. Sorting the values 
		// with this key yields Gaussian IDs in a list, such that they
		// are first sorted by tile and then by depth. 
		for (int y = rect_min.y; y < rect_max.y; y++)
		{
			for (int x = rect_min.x; x < rect_max.x; x++)
			{
				uint64_t key = y * grid.x + x;
				key <<= 32;
				key |= *((uint32_t*)&depths[idx]);
				gaussian_keys_unsorted[off] = key;
				gaussian_values_unsorted[off] = idx;
				off++;
			}
		}
	}
}

// Check keys to see if it is at the start/end of one tile's range in 
// the full sorted list. If yes, write start/end of this tile. 
// Run once per instanced (duplicated) Gaussian ID.
__global__ void identifyTileRanges(int L, uint64_t* point_list_keys, uint2* ranges)
{
	// Use standard thread indexing instead of cg::this_grid() for AMD compatibility
	int idx = blockIdx.x * blockDim.x + threadIdx.x;
	if (idx >= L)
		return;

	// Read tile ID from key. Update start/end of tile range if at limit.
	uint64_t key = point_list_keys[idx];
	uint32_t currtile = key >> 32;
	if (idx == 0)
		ranges[currtile].x = 0;
	else
	{
		uint32_t prevtile = point_list_keys[idx - 1] >> 32;
		if (currtile != prevtile)
		{
			ranges[prevtile].y = idx;
			ranges[currtile].x = idx;
		}
	}
	if (idx == L - 1)
		ranges[currtile].y = L;
}

// Mark Gaussians as visible/invisible, based on view frustum testing
void CudaRasterizer::Rasterizer::markVisible(
	int P,
	float* means3D,
	float* viewmatrix,
	float* projmatrix,
	bool* present)
{
	hipLaunchKernelGGL(checkFrustum, (P + 255) / 256, 256, 0, 0, 
		P,
		means3D,
		viewmatrix, projmatrix,
		present);
}

CudaRasterizer::GeometryState CudaRasterizer::GeometryState::fromChunk(char*& chunk, size_t P)
{
	GeometryState geom;
	geom.scan_size = 0;  // Initialize to 0 for hipcub compatibility
	obtain(chunk, geom.depths, P, 128);
	obtain(chunk, geom.clamped, P * 3, 128);
	obtain(chunk, geom.internal_radii, P, 128);
	obtain(chunk, geom.means2D, P, 128);
	obtain(chunk, geom.cov3D, P * 6, 128);
	obtain(chunk, geom.conic_opacity, P, 128);
	obtain(chunk, geom.rgb, P * 3, 128);
	obtain(chunk, geom.tiles_touched, P, 128);
	hipcub::DeviceScan::InclusiveSum(nullptr, geom.scan_size, geom.tiles_touched, geom.tiles_touched, P);
	obtain(chunk, geom.scanning_space, geom.scan_size, 128);
	obtain(chunk, geom.point_offsets, P, 128);
	return geom;
}

CudaRasterizer::ImageState CudaRasterizer::ImageState::fromChunk(char*& chunk, size_t N)
{
	ImageState img;
	obtain(chunk, img.accum_alpha, N, 128);
	obtain(chunk, img.n_contrib, N, 128);
	obtain(chunk, img.ranges, N, 128);
	return img;
}

CudaRasterizer::BinningState CudaRasterizer::BinningState::fromChunk(char*& chunk, size_t P)
{
	BinningState binning;
	binning.sorting_size = 0;  // Initialize to 0 for hipcub compatibility
	obtain(chunk, binning.point_list, P, 128);
	obtain(chunk, binning.point_list_unsorted, P, 128);
	obtain(chunk, binning.point_list_keys, P, 128);
	obtain(chunk, binning.point_list_keys_unsorted, P, 128);
	hipcub::DeviceRadixSort::SortPairs(
		nullptr, binning.sorting_size,
		binning.point_list_keys_unsorted, binning.point_list_keys,
		binning.point_list_unsorted, binning.point_list, P);
	obtain(chunk, binning.list_sorting_space, binning.sorting_size, 128);
	return binning;
}

// Forward rendering procedure for differentiable rasterization
// of Gaussians.
int CudaRasterizer::Rasterizer::forward(
	std::function<char* (size_t)> geometryBuffer,
	std::function<char* (size_t)> binningBuffer,
	std::function<char* (size_t)> imageBuffer,
	const int P, int D, int M,
	const float* background,
	const int width, int height,
	const float* means3D,
	const float* shs,
	const float* colors_precomp,
	const float* opacities,
	const float* scales,
	const float scale_modifier,
	const float* rotations,
	const float* cov3D_precomp,
	const float* viewmatrix,
	const float* projmatrix,
	const float* cam_pos,
	const float tan_fovx, float tan_fovy,
	const float kernel_size,
	const float* subpixel_offset,
	const bool prefiltered,
	float* out_color,
	int* radii,
	bool debug)
{
	const float focal_y = height / (2.0f * tan_fovy);
	const float focal_x = width / (2.0f * tan_fovx);

	size_t chunk_size = required<GeometryState>(P);
	char* chunkptr = geometryBuffer(chunk_size);
	GeometryState geomState = GeometryState::fromChunk(chunkptr, P);

	if (radii == nullptr)
	{
		radii = geomState.internal_radii;
	}

	dim3 tile_grid((width + BLOCK_X - 1) / BLOCK_X, (height + BLOCK_Y - 1) / BLOCK_Y, 1);
	dim3 block(BLOCK_X, BLOCK_Y, 1);

	// Dynamically resize image-based auxiliary buffers during training
	size_t img_chunk_size = required<ImageState>(width * height);
	char* img_chunkptr = imageBuffer(img_chunk_size);
	ImageState imgState = ImageState::fromChunk(img_chunkptr, width * height);

	if (NUM_CHANNELS != 3 && colors_precomp == nullptr)
	{
		throw std::runtime_error("For non-RGB, provide precomputed Gaussian colors!");
	}

	// DEBUG: Zero the tiles_touched buffer BEFORE the kernel
	// If we still see zeros after kernel, kernel isn't writing
	// If we see garbage, kernel is corrupting or pointers are wrong
	hipMemset(geomState.tiles_touched, 0, P * sizeof(uint32_t));
	
	// CRITICAL: Wait for memset to complete
	hipDeviceSynchronize();
	
	// Now run the preprocess kernel
	CHECK_CUDA(FORWARD::preprocess(
		P, D, M,
		means3D,
		(glm::vec3*)scales,
		scale_modifier,
		(glm::vec4*)rotations,
		opacities,
		shs,
		geomState.clamped,
		cov3D_precomp,
		colors_precomp,
		viewmatrix, projmatrix,
		(glm::vec3*)cam_pos,
		width, height,
		focal_x, focal_y,
		tan_fovx, tan_fovy,
		kernel_size,
		radii,
		geomState.means2D,
		geomState.depths,
		geomState.cov3D,
		geomState.rgb,
		geomState.conic_opacity,
		tile_grid,
		geomState.tiles_touched,
		prefiltered
	), debug)

	// CRITICAL: Wait for preprocess kernel to complete before reading results
	hipDeviceSynchronize();
	
	// DEBUG: Check for kernel launch errors
	hipError_t kernelErr = hipGetLastError();
	if (kernelErr != hipSuccess) {
		std::cerr << "[HIP KERNEL ERROR] preprocessCUDA failed: " << hipGetErrorString(kernelErr) << std::endl;
	} else {
		std::cerr << "[HIP KERNEL] preprocessCUDA launched successfully, P=" << P << std::endl;
	}

	// DEBUG: Read what preprocessCUDA actually wrote (non-destructive)
	uint32_t preprocess_output[10];
	hipMemcpy(preprocess_output, geomState.tiles_touched, 10 * sizeof(uint32_t), hipMemcpyDeviceToHost);
	std::cerr << "[HIP DEBUG] preprocessCUDA output (first 10): ";
	for(int i = 0; i < 10; i++) {
		std::cerr << preprocess_output[i] << " ";
	}
	std::cerr << std::endl;

	// Compute prefix sum over full list of touched tile counts by Gaussians
	// E.g., [2, 3, 0, 2, 1] -> [2, 5, 5, 7, 8]
	// Using custom scan kernel since hipcub has issues on AMD GPUs
	customInclusiveScan(geomState.tiles_touched, geomState.point_offsets, P);

	// Retrieve total number of Gaussian instances to launch and resize aux buffers
	int num_rendered;
	CHECK_CUDA(hipMemcpy(&num_rendered, geomState.point_offsets + P - 1, sizeof(int), hipMemcpyDeviceToHost), debug);

	// Debug and sanity check for HIP - prevent garbage allocation
	if (debug || num_rendered < 0 || num_rendered > P * 1000) {
		std::cerr << "[HIP DEBUG] P=" << P << ", num_rendered=" << num_rendered << std::endl;
	}
	if (num_rendered < 0 || num_rendered > P * 1000) {
		std::cerr << "[HIP ERROR] num_rendered=" << num_rendered << " is invalid (P=" << P << "). Setting to P." << std::endl;
		num_rendered = P;  // Fallback to safe value
	}

	size_t binning_chunk_size = required<BinningState>(num_rendered);
	char* binning_chunkptr = binningBuffer(binning_chunk_size);
	BinningState binningState = BinningState::fromChunk(binning_chunkptr, num_rendered);

	// For each instance to be rendered, produce adequate [ tile | depth ] key 
	// and corresponding dublicated Gaussian indices to be sorted
	
	// Initialize buffers to detect if kernel writes or leaves uninitialized
	hipMemset(binningState.point_list_keys_unsorted, 0, num_rendered * sizeof(uint64_t));
	hipMemset(binningState.point_list_unsorted, 0, num_rendered * sizeof(uint32_t));
	hipDeviceSynchronize();
	
	std::cerr << "[HIP DEBUG] Launching duplicateWithKeys with num_rendered=" << num_rendered << std::endl;
	duplicateWithKeys<<<(P + 255) / 256, 256>>>(
		P,
		geomState.means2D,
		geomState.depths,
		geomState.point_offsets,
		binningState.point_list_keys_unsorted,
		binningState.point_list_unsorted,
		radii,
		tile_grid);
	hipDeviceSynchronize();
	hipError_t dupErr = hipGetLastError();
	if (dupErr != hipSuccess) {
		std::cerr << "[HIP ERROR] duplicateWithKeys failed: " << hipGetErrorString(dupErr) << std::endl;
	} else {
		std::cerr << "[HIP DEBUG] duplicateWithKeys completed successfully" << std::endl;
	}

	int bit = getHigherMsb(tile_grid.x * tile_grid.y);
	
	// Sample UNSORTED keys before sorting to check if duplicateWithKeys produced garbage
	{
		uint64_t unsortedFirst[5], unsortedLast[5];
		hipMemcpy(unsortedFirst, binningState.point_list_keys_unsorted, 5 * sizeof(uint64_t), hipMemcpyDeviceToHost);
		int endOff = num_rendered - 5;
		hipMemcpy(unsortedLast, binningState.point_list_keys_unsorted + endOff, 5 * sizeof(uint64_t), hipMemcpyDeviceToHost);
		
		std::cerr << "[HIP DEBUG] UNSORTED keys (first 5 tile IDs): ";
		for(int i = 0; i < 5; i++) { std::cerr << (unsortedFirst[i] >> 32) << " "; }
		std::cerr << std::endl;
		
		std::cerr << "[HIP DEBUG] UNSORTED keys (last 5 tile IDs @" << endOff << "): ";
		for(int i = 0; i < 5; i++) { std::cerr << (unsortedLast[i] >> 32) << " "; }
		std::cerr << std::endl;
	}
	
	std::cerr << "[HIP DEBUG] Launching SortPairs with num_rendered=" << num_rendered << ", bit=" << bit << std::endl;

	// Sort complete list of (duplicated) Gaussian indices by keys
	CHECK_CUDA(hipcub::DeviceRadixSort::SortPairs(
		binningState.list_sorting_space,
		binningState.sorting_size,
		binningState.point_list_keys_unsorted, binningState.point_list_keys,
		binningState.point_list_unsorted, binningState.point_list,
		num_rendered, 0, 32 + bit), debug)
	
	hipDeviceSynchronize();
	std::cerr << "[HIP DEBUG] SortPairs completed" << std::endl;

	std::cerr << "[HIP DEBUG] hipMemset for ranges" << std::endl;
	CHECK_CUDA(hipMemset(imgState.ranges, 0, tile_grid.x * tile_grid.y * sizeof(uint2)), debug);
	hipDeviceSynchronize();
	std::cerr << "[HIP DEBUG] hipMemset completed" << std::endl;

	// Identify start and end of per-tile workloads in sorted list
	if (num_rendered > 0) {
		// Debug: check tile_grid dimensions and sample some keys
		int numTiles = tile_grid.x * tile_grid.y;
		int numBlocks = (num_rendered + 255) / 256;
		std::cerr << "[HIP DEBUG] identifyTileRanges: num_rendered=" << num_rendered 
				  << ", numBlocks=" << numBlocks 
				  << ", tile_grid=(" << tile_grid.x << "x" << tile_grid.y << ")=" << numTiles 
				  << std::endl;
		
		// Sample first few keys to validate
		uint64_t sampleKeys[5];
		hipMemcpy(sampleKeys, binningState.point_list_keys, 5 * sizeof(uint64_t), hipMemcpyDeviceToHost);
		std::cerr << "[HIP DEBUG] Sample sorted keys (first 5 tile IDs): ";
		for(int i = 0; i < 5; i++) {
			uint32_t tileId = sampleKeys[i] >> 32;
			std::cerr << tileId << " ";
		}
		std::cerr << std::endl;
		
		// Sample keys from near the end where garbage might be
		if (num_rendered > 10) {
			uint64_t endKeys[5];
			int endOffset = num_rendered - 5;
			hipMemcpy(endKeys, binningState.point_list_keys + endOffset, 5 * sizeof(uint64_t), hipMemcpyDeviceToHost);
			std::cerr << "[HIP DEBUG] Sample sorted keys (last 5 tile IDs @" << endOffset << "): ";
			int maxTileId = 0;
			for(int i = 0; i < 5; i++) {
				uint32_t tileId = endKeys[i] >> 32;
				std::cerr << tileId << " ";
				if (tileId > maxTileId) maxTileId = tileId;
			}
			std::cerr << std::endl;
			if (maxTileId >= numTiles) {
				std::cerr << "[HIP ERROR] maxTileId=" << maxTileId << " >= numTiles=" << numTiles << " - garbage keys!" << std::endl;
			}
		}
		
		std::cerr << "[HIP DEBUG] Launching identifyTileRanges" << std::endl;
		
		// Validate pointers
		std::cerr << "[HIP DEBUG] point_list_keys ptr: " << (void*)binningState.point_list_keys << std::endl;
		std::cerr << "[HIP DEBUG] ranges ptr: " << (void*)imgState.ranges << std::endl;
		
		// Only launch if pointers are valid
		if (binningState.point_list_keys == nullptr || imgState.ranges == nullptr) {
			std::cerr << "[HIP ERROR] Null pointer in identifyTileRanges!" << std::endl;
		} else {
			identifyTileRanges<<<(num_rendered + 255) / 256, 256>>>(
				num_rendered,
				binningState.point_list_keys,
				imgState.ranges);
			hipDeviceSynchronize();
			hipError_t rangeErr = hipGetLastError();
			if (rangeErr != hipSuccess) {
				std::cerr << "[HIP ERROR] identifyTileRanges failed: " << hipGetErrorString(rangeErr) << std::endl;
			} else {
				std::cerr << "[HIP DEBUG] identifyTileRanges completed successfully" << std::endl;
			}
		}
	}

	// Let each tile blend its range of Gaussians independently in parallel
	const float* feature_ptr = colors_precomp != nullptr ? colors_precomp : geomState.rgb;
	CHECK_CUDA(FORWARD::render(
		tile_grid, block,
		imgState.ranges,
		binningState.point_list,
		width, height,
		(float2*)subpixel_offset,
		geomState.means2D,
		feature_ptr,
		geomState.conic_opacity,
		imgState.accum_alpha,
		imgState.n_contrib,
		background,
		out_color), debug)

	return num_rendered;
}

// Produce necessary gradients for optimization, corresponding
// to forward render pass
void CudaRasterizer::Rasterizer::backward(
	const int P, int D, int M, int R,
	const float* background,
	const int width, int height,
	const float* means3D,
	const float* shs,
	const float* colors_precomp,
	const float* scales,
	const float scale_modifier,
	const float* rotations,
	const float* cov3D_precomp,
	const float* viewmatrix,
	const float* projmatrix,
	const float* campos,
	const float tan_fovx, float tan_fovy,
	const float kernel_size,
	const float* subpixel_offset,
	const int* radii,
	char* geom_buffer,
	char* binning_buffer,
	char* img_buffer,
	const float* dL_dpix,
	float* dL_dmean2D,
	float* dL_dconic,
	float* dL_dopacity,
	float* dL_dcolor,
	float* dL_dmean3D,
	float* dL_dcov3D,
	float* dL_dsh,
	float* dL_dscale,
	float* dL_drot,
	bool debug)
{
	GeometryState geomState = GeometryState::fromChunk(geom_buffer, P);
	BinningState binningState = BinningState::fromChunk(binning_buffer, R);
	ImageState imgState = ImageState::fromChunk(img_buffer, width * height);

	if (radii == nullptr)
	{
		radii = geomState.internal_radii;
	}

	const float focal_y = height / (2.0f * tan_fovy);
	const float focal_x = width / (2.0f * tan_fovx);

	const dim3 tile_grid((width + BLOCK_X - 1) / BLOCK_X, (height + BLOCK_Y - 1) / BLOCK_Y, 1);
	const dim3 block(BLOCK_X, BLOCK_Y, 1);

	// Compute loss gradients w.r.t. 2D mean position, conic matrix,
	// opacity and RGB of Gaussians from per-pixel loss gradients.
	// If we were given precomputed colors and not SHs, use them.
	const float* color_ptr = (colors_precomp != nullptr) ? colors_precomp : geomState.rgb;
	CHECK_CUDA(BACKWARD::render(
		tile_grid,
		block,
		imgState.ranges,
		binningState.point_list,
		width, height,
		(float2*)subpixel_offset,
		background,
		geomState.means2D,
		geomState.conic_opacity,
		color_ptr,
		imgState.accum_alpha,
		imgState.n_contrib,
		dL_dpix,
		(float3*)dL_dmean2D,
		(float4*)dL_dconic,
		dL_dopacity,
		dL_dcolor), debug)

	// Take care of the rest of preprocessing. Was the precomputed covariance
	// given to us or a scales/rot pair? If precomputed, pass that. If not,
	// use the one we computed ourselves.
	const float* cov3D_ptr = (cov3D_precomp != nullptr) ? cov3D_precomp : geomState.cov3D;
	CHECK_CUDA(BACKWARD::preprocess(P, D, M,
		(float3*)means3D,
		radii,
		shs,
		geomState.clamped,
		(glm::vec3*)scales,
		(glm::vec4*)rotations,
		scale_modifier,
		cov3D_ptr,
		viewmatrix,
		projmatrix,
		focal_x, focal_y,
		tan_fovx, tan_fovy,
		kernel_size,
		(glm::vec3*)campos,
		(float3*)dL_dmean2D,
		dL_dconic,
		(glm::vec3*)dL_dmean3D,
		dL_dcolor,
		dL_dcov3D,
		dL_dsh,
		(glm::vec3*)dL_dscale,
		(glm::vec4*)dL_drot,
		geomState.conic_opacity,
		dL_dopacity), debug)
}